{"cells":[{"cell_type":"markdown","metadata":{"id":"PSIz_CRfm_ly"},"source":["# A simple recurrent network\n","\n","In this tutorial we will learn how to build, train, and test a simple recurrent network (SRN) in the context of the AXBY task. We will do this using python, and the focus of the tutorial is on building an SRN using the tensorflow (via keras) deep learning library. We will first motivate the use of an SRN for the task by showing an alternative implementation using basic python functions.\n","\n","This tutorial assumes you have read the earlier mini-tutorial on feed forward networks. If you are not familiar with feed forward networks, you should first familiarize yourself with the course materials on those simpler networks."]},{"cell_type":"markdown","metadata":{"id":"KV6jlCzhm_l0"},"source":["## AXBY task\n","SRNs are networks that take inputs and generate outputs over several sequential steps of time. Many human behaviors we study in cognitive science have this character. SRNs have been instrumental in developing theories in several cognitive domains, including working memory and cognitive control, memory for arbitrary serial order, sequential comprehension and production of single words, and sentence comprehension and production. A very useful resource for understanding the core interest and basic operation of SRNs is Jeff Elman’s classic paper from 1990 in Cognitive Science called [\"Finding Structure in Time\"](https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1).\n","\n","In the standard AX task, people view a sequence of letters and are instructed to press a button whenever an X appears, UNLESS the X was preceded by an A, in which case they should NOT press the button. This task, though very simple, draws out some important characteristics of cognitive control and working memory. Specifically, to perform correctly the participant must (a) retain a representation of a prior event (the preceding letter), (b) continually update this representation with each new letter in the sequence and (c) use the representation of the prior event to constrain the response generated to the current item. In particular, when an X appears following an A, the participant must use the “memory” of the preceding A to *inhibit* the response usually associated with the X (the button press).\n","\n","### Instructions\n","The task variant we will study here adds a further level of complexity. In the AXBY task, participants must press button 1 whenever an X appears, UNLESS the X was preceded by an A in which case no button should be pressed, UNLESS the A was preceded by the letter B, in which case button 2 should be pressed. This task thus requires the participant to retain and continually update information about the preceding two letters in the sequence, and to use this conjoint representation to shape the response generated for the X (button 2 if preceded by B then A; otherwise no button if preceded by A; otherwise button 1). Extensive research on tasks similar to this have suggested that these abilities are supported by structures in the prefrontal cortex, and that behaviors on such tasks can be understand by SRN-like mechanisms.\n","\n","### Intuition\n","\n","One possible intuitive solution to modeling this process (without using an artificial neural network) is to store the stimuli in a 3-slot memory buffer. Based on the pattern in the buffer, decide whether to press button 1, 2, or not to press any button (0).\n","\n","All possibilities:\n","\n","    - 9 possible patterns in the buffer ending with X:\n","\n","        - A, A, X -> 0\n","        - A, B, X -> 1\n","        - A, X, X -> 1\n","        - B, A, X -> 2\n","        - B, B, X -> 1\n","        - B, X, X -> 1\n","        - X, A, X -> 0\n","        - X, B, X -> 1\n","        - X, X, X -> 1\n","        \n","    - all other patterns\n","\n","        - ?, ?, not X -> 0"]},{"cell_type":"markdown","metadata":{"id":"2ssix3eim_l1"},"source":["## Solve AXBY task using basic python functions and simple data structures"]},{"cell_type":"markdown","metadata":{"id":"mYrOHQT3m-gu"},"source":["Due to the simplicity of this task, it is not difficult to solve it with some basic python programming, essentially relying on conditionals. The following code would be one such solution to this task. \n","\n","*NOTE:* The functions and objects created in this section will be used later when developing the neural network, so make sure to run the cells!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J70c8n5MIAXu"},"outputs":[],"source":["def do_task(inputs):\n","    \"\"\"Function to process AXBY task.\n","    inputs: A list with three elements indicating\n","    three successive stimuli in the task.\"\"\"\n","\n","    last_last_input = None\n","    last_input = None\n","    outputs = []\n","\n","    for input in inputs:\n","        if input == 'X':\n","            if last_input == 'A':\n","                if last_last_input == 'B':\n","                    outputs.append(2)\n","                else:\n","                    outputs.append(0)\n","            else:\n","                outputs.append(1)\n","        else:\n","            outputs.append(0)\n","\n","        last_last_input = last_input\n","        last_input = input\n","\n","    return outputs\n"]},{"cell_type":"markdown","metadata":{"id":"AMjuPnJQm_l4"},"source":["Let's test the function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9mfzt7Ihm_l4"},"outputs":[],"source":["# test_cases is a list containing all the inputs and expected outputs\n","# each element of the test_cases list is a tuple (11 total)\n","# the first element of each tuple is a list with the input (e.g., ['A', 'A', 'X'])\n","# the second element is an integer expressing the expected output (0, 1, or 2)\n","test_cases = [\n","    (['A', 'A', 'X'], 0), \n","    (['A', 'B', 'X'], 1), \n","    (['A', 'X', 'X'], 1), \n","    (['B', 'A', 'X'], 2), \n","    (['B', 'B', 'X'], 1),\n","    (['B', 'X', 'X'], 1),\n","    (['X', 'A', 'X'], 0),\n","    (['X', 'B', 'X'], 1),\n","    (['X', 'X', 'X'], 1),\n","    (['X', 'X', 'B'], 0),\n","    (['X', 'X', 'A'], 0),\n","]\n","\n","# write a loop to apply do_task() to\n","for test_case in test_cases:\n","    inputs, expected_output = test_case\n","    output = do_task(inputs)\n","    last_output = output[-1]\n","    print(f\"{inputs=}, {output=}, {last_output=}, {expected_output=}, {last_output==expected_output=}\")"]},{"cell_type":"markdown","metadata":{"id":"Mq1KxC0Em_l5"},"source":["All the tests pass. Just to make sure, let's randomly generate some cases and double-check."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZkEPKDmm_l5"},"outputs":[],"source":["import pandas as pd\n","from random import choices\n","\n","\n","seq_len = 20 # create a desired sequence length\n","inputs = choices(['X', 'A', 'B'], k=seq_len) # select that many stimuli from our three possible\n","pd.DataFrame({'inputs': inputs, 'outputs': do_task(inputs)}).T # create a data-frame with the inputs/ outputs\n","# for more information about what is going on in the above line, look up \"dictionary comprehension\""]},{"cell_type":"markdown","metadata":{"id":"AyRXvrKPm_l6"},"source":["Looks like this approach works - all the outputs are what is required based on the task description for AXBY. But this type of approach is not scalable to more complex problem. e.g., computer vision, speech recognition, etc."]},{"cell_type":"markdown","metadata":{"id":"dbnJByJcm_l7"},"source":["## Solve AXBY task with RNN\n","Let's now approach the task with a recurrent neural network.\n","\n","We can begin with a schematic drawing of a recurrent neural network that can solve this sequential task. We will go into detail about the network later, but use this simple illustration to guide our thinking.\n","\n","\n","<br>\n","<img src='https://drive.google.com/uc?id=1dhsDSGK7Af_Zg9Y0TEjbRvzvLdRxErV4'>\n","\n","\n","The model gets direct input for the currently-viewed letter, which in this case can be X, A or B. Input units send feed-forward connections to a hidden layer, which in turn project to an output layer containing two units. The first unit indicates a press for button 1; the second indicates a press for button 2. If no button is to be pressed, neither unit should be activated. Note that all of these connections are feed forward.\n","\n","Hanging off the side of the hidden layer is a second layer of units, labeled “Context.” The context units, like the input, send feed-forward connections to the hidden layer. However, they also _receive_ connections from the hidden layer, which are shown as a dotted line labeled t-1. These connections play a special role: rather than transmitting information from sending to receiving units via weighted connections in the usual way, they instead simply _copy_ the activation pattern from the previous point in time to the units in the receiving units. It is these connections that makes the network “recurrent.” The hidden layer and context layer together are sometimes jointly referred to as a “recurrent” layer.\n","\n","Note that the context layer has the same number of units as the hidden layer, and simply contains a copy of the hidden layer activation pattern on the previous timestep. They can be viewed as containing a \"memory\" of the previous hidden state. The weights projecting from context to hidden layers act just like all other weights in the network, influencing the pattern of activation on the current time step (together with the current input and the bias). These weights can be viewed as capturing how a \"memory\" of the prior hidden pattern should influence the current pattern.\n"]},{"cell_type":"markdown","metadata":{"id":"8nxfjMLmm_l8"},"source":["### Data preparation (manually, showing all steps)"]},{"cell_type":"markdown","metadata":{"id":"o-hhVeYSQJnM"},"source":["First import these modules to aid in constructing the input and output patterns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAtfrWF9m_l7"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"bis5X9a6QWRI"},"source":["Let's look at the sequence of inputs for the task generated in the earlier code block:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eYe5xGOim_l8"},"outputs":[],"source":["print(f\"{inputs=}\")"]},{"cell_type":"markdown","metadata":{"id":"qOQOAOsdm_l8"},"source":["1. Encode these inputs into numbers:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktUIKoQGm_l9"},"outputs":[],"source":["input_mapping = {'X': 0, 'A': 1, 'B': 2}\n","numeric_inputs = [input_mapping[input] for input in inputs]\n","print(f\"{numeric_inputs=}\")"]},{"cell_type":"markdown","metadata":{"id":"mKXEgRfnm_l9"},"source":["2. Create one-hot encodings of inputs\n","\n","You can see from the printed array below that each list of binary values has one unit set to `1` (the \"hot\" unit/ node) and the others set to `0`. The representation for the numeric input `0` has the 0th unit set to `1`, the rep for input `1` has the 1st unit set to `1`, and the rep for `2` has the 2nd unit set to `1`. (Remember that in `python` we always call the 1st element in a thing the 0th element, and `0` is its index in that thing)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_nkQROCm_l9"},"outputs":[],"source":["one_hot_inputs = tf.one_hot(numeric_inputs, depth=len(input_mapping))\n","print(f\"{one_hot_inputs=}\")"]},{"cell_type":"markdown","metadata":{"id":"rAXhYN_Am_l-"},"source":["#### Create the input pipeline\n","When we create workflows in `python` (and other languages) it is best to automate processes as much as possible. Here, let's rewrite the code above (i.e., we *refactor* our previous code) into better code that automates the process. We call an automated process like this a *pipeline*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kYgIN10Wm_l-"},"outputs":[],"source":["def get_case(seq_len=20, verbose=False):\n","    \"\"\"Create a case of inputs and outputs.\"\"\"\n","\n","    input_mapping = {'X': 0, 'A': 1, 'B': 2}\n","\n","    # Input\n","    input_tokens = list(input_mapping.keys())\n","    inputs = choices(input_tokens, k=seq_len)\n","    encoded_inputs = [input_mapping[input] for input in inputs]\n","    tensor_inputs = tf.one_hot(encoded_inputs, depth=len(input_mapping))\n","\n","    # Input representation:\n","    # X: [1, 0, 0]\n","    # A: [0, 1, 0]\n","    # B: [0, 0, 1]\n","    \n","    # Output (similar to input, except we remove the first bit)  \n","    expected_outputs = do_task(inputs)\n","    tensor_outputs = tf.one_hot(expected_outputs, depth=3)\n","    tensor_outputs = tensor_outputs[:, 1:]  # Remove the first bit. *Technically, not necessary, but it makes the output more intuitive.*\n","\n","    # output representation: \n","    # No button press: [0, 0]\n","    # Button 1: [1, 0]\n","    # Button 2: [0, 1]\n","    \n","    if verbose:\n","        print(f\"{inputs=}\")\n","        print(f\"{expected_outputs=}\")\n","\n","    return tensor_inputs, tensor_outputs\n","\n","x, y = get_case(verbose=True)\n","print(f\"{x.shape=} and {y.shape=}\")\n"]},{"cell_type":"markdown","metadata":{"id":"xa9AEEtQm_l-"},"source":["Conceptually we often imagine that a network computes error and adjusts weights after processing a single training example, a mode sometimes calls *online learning.* Online learning is often pretty slow though! In practice it is more common to present a model with a *batch* of training examples&mdash;not a single pattern, and not the entire corpus, but a random sampling of *n* patterns from the corpus, whene *n* is a free hyperparameter. The model processes all patterns in the batch, tabulating the error and weight gradients for each. After all patterns in the batch have been processes, the gradients on each weight are summed together (across patterns), and a single weight update is made. This speeds things up considerably, and also ensures that a single pattern does not have a giant impact on the weight update. \n","\n","Let's make a function to get a batch:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VySJnhWVm_l_"},"outputs":[],"source":["def get_cases(n, verbose=False):\n","    \"\"\"Get a batch of cases.\"\"\"\n","    \n","    cases = [get_case(verbose=verbose) for _ in range(n)]\n","    inputs = tf.stack([case[0] for case in cases])\n","    outputs = tf.stack([case[1] for case in cases])\n","    return inputs, outputs\n","\n","batch_x, batch_y = get_cases(5)\n","print(f\"{batch_x.shape=} and {batch_y.shape=}\")\n"]},{"cell_type":"markdown","metadata":{"id":"xmNGVWkwm_l_"},"source":["The above code shows the shape of the batch objects for the input (batch_x) and output (batch_y) patterns. Recall that (1) there are three input units, (2) we have designed sequences of length 20 for the AXBY task, and the code shown asks for batches of 5 patterns. You can see that the input batch array is of shape [5, 20, 3]&mdash;so you should infer that the first dimension of the array inindicates different patterns in the batch, the second dimension indicates different time-points in the sequence, and the third indicates differen input units. This is the standard arrangement for batch input and output arrays for sequential models in Keras/TensorFlow.\n","\n","You can see the full sequence of inputs for the first item in the batch just by indexing the array as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NipGsQP7JOQs"},"outputs":[],"source":["batch_x[0,:,:]"]},{"cell_type":"markdown","metadata":{"id":"S4O2tJUOm_l_"},"source":["## Constructing and training the RNN model with built-in tools"]},{"cell_type":"markdown","metadata":{"id":"PDH0QX2w3B72"},"source":["The Tensorflow/KERAS ecosystem has many convenient functions for building and training RNNs. In this section we will use the `keras.layers.SimpleRNN` function to build the model, and the built-in *fit* method to train it.\n","You can read the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN) for details. To better understand how learning and processing works in a simple RNN, the next section will construct similar functionality from scratch, with a more considered explanation of the underlying computations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjBfxaQ-3B72"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import SimpleRNN, Dense\n","from tensorflow.keras.models import Sequential\n","\n","# Define the shape of the input, output, and sequence length\n","seq_len = 20\n","input_units = 3\n","output_units = 2\n","\n","# Create a sequential model, i.e., a simple stack of layers, one after the other:\n","easy_model = Sequential()  \n","\n","# Add a simple RNN layer with 10 units. This is the combined hidden-and-context layer from the earlier figure.\n","#The input shape is (20, 3), i.e., 20 time steps, each with 3 inputs matching our input shape (we use 3 units to represent X, A, B).\n","easy_model.add(SimpleRNN(units=10, input_shape=(seq_len, input_units), return_sequences=True)) \n","\n","# Add a dense layer for the output units.\n","easy_model.add(Dense(output_units, activation='sigmoid'))  \n","\n","easy_model.summary()  # Print a summary of the model."]},{"cell_type":"markdown","metadata":{"id":"7M0GL_mjm-g8"},"source":["In this method of building a model, the first layer added is interpreted as receiving inputs, so input shape must be specified. Each subsequent layer is assumed to receive inputs from the previously-created layer. Here the whole model is created with three lines of code!\n","\n","Training the model then involves the same steps employed for feed-forward models:\n","\n","1. Define the loss function and the optimizer\n","2. `Compile` the model\n","3. `Fit` the model with data for some number of (`n`) *epochs* (the number of times the model sees the entire dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsStTDQ0m-g8"},"outputs":[],"source":["epochs = 100\n","x_train, y_train = get_cases(100)\n","\n","loss_fn = tf.keras.losses.BinaryCrossentropy()\n","optimizer = tf.keras.optimizers.SGD(learning_rate=1)  # Using a large learning rate for this simple toy problem\n","\n","easy_model.compile(optimizer=optimizer, loss=loss_fn)\n","easy_model.fit(x_train, y_train, epochs=epochs)"]},{"cell_type":"markdown","metadata":{"id":"Ra5Ij2cB3B72"},"source":["### Building an RNN with custom classes.\n","\n","To better understand how processing and training work in this model, we will build a model with similar functionality via custom classes and functions.\n","\n","Before beginning, it is helpful to visualize the information flow in this model in a more detailed illustration, together with some equations to help you map aspects of the computation (and the diagram) to the code. First, here is a more detailed diagram of an RNN \"cell\": the combination of hidden and context units shown in simple form earlier:"]},{"cell_type":"markdown","metadata":{"id":"oBZNu0LqI-Ht"},"source":["![model architechture](https://drive.google.com/uc?id=1f3kI6XCF6vrtiQFQG1PvpQbTNqbim7uT)\n"]},{"cell_type":"markdown","metadata":{"id":"mvYIvX-7Vysw"},"source":["The boxes labelled *RNNCell* contain all of the hidden/context units, while the other chart elements indicate all of the computations carried out by these units in a given timestep. Focusing on time *t*, you can see the units in the cell receive inputs from the hidden units in the previous timestep (arrow labelled $w_{hh}$), from the current input pattern (arrow labelled $x_t$), and from the bias unit $b$. The circle with a plus-sign indicates that these inputs are summed together, while the $\\sigma$ indicates that the sum is passed through the sigmoid activation function. These computations are carried out for each hidden unit, creating the tensor of output activations labelled $h_t$. The output activations are passed forward through a layer of weights to the output units at time $t$ (not shown), and also provide input to the hidden units at the next timestep $t+1$ via the weights $w_{hh}$. The entire sequence can then be viewed as iterating this process for $n$ steps, unrolling the network forward in time.\n","\n","With this understanding, let's create this functionality from more elementary building blocks. First we define an RNNCell class, which defines the computations carried out in a single timestep: "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w0YKeudUm_mA"},"outputs":[],"source":["class RNNCell(tf.keras.layers.Layer):\n","    \"\"\"Simple RNN cell.\"\"\"\n","\n","    def __init__(self, units):\n","        super().__init__()\n","        self.units = units\n","\n","    def build(self, input_shape):\n","        self.input_dense = tf.keras.layers.Dense(self.units, use_bias=False) # w_xh\n","        self.recurrent_dense = tf.keras.layers.Dense(self.units, use_bias=False)  # w_hh\n","        self.bias = self.add_weight(shape=(self.units,), name='bias')  # b\n","        self.built = True # attach an attribute that shows that the network has been built\n","\n","    def call(self, inputs, states):\n","        hx = self.input_dense(inputs)  # x @ w_xh (@: matrix multiplication)\n","        hh = self.recurrent_dense(states)  # h @ w_hh\n","        outputs = hx + hh + self.bias\n","        outputs = tf.sigmoid(outputs)\n","        return outputs, outputs  # technically outputs, states, but they are the same\n"]},{"cell_type":"markdown","metadata":{"id":"C-hlstxG3B73"},"source":["Next we create a `SimpleRNN` class that loops through RNNCell computations in each step of a sequence:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Wv_Cs6Jm_mA"},"outputs":[],"source":["class SimpleRNN(tf.keras.layers.Layer):\n","    \"\"\"Simple RNN Layer.\"\"\"\n","\n","    def __init__(self, units):\n","        super().__init__()\n","        self.units = units\n","\n","    def build(self, input_shape):\n","        self.rnn_cell = RNNCell(units=self.units)\n","        self.built = True  # Boiler-plate\n","\n","    def call(self, inputs, states=None):\n","\n","        batch_size = tf.shape(inputs)[0]\n","        seq_len = tf.shape(inputs)[1]\n","\n","        # Rule of thumb: do not use python lists inside a keras layer's call method\n","        # tf.TensorArray is a better alternative, see docs for details\n","        outputs = tf.TensorArray(tf.float32, size=seq_len)  \n","\n","        # Initialize the hidden state\n","        states = tf.zeros([batch_size, self.units])\n","\n","        for t in range(seq_len):\n","            this_input = inputs[:, t, :]\n","            states, output = self.rnn_cell(inputs=this_input, states=states)\n","            outputs = outputs.write(t, output)  # Analogous to list.append()\n","\n","        outputs = outputs.stack()  # output with wrong shape (seq_len, batch_size, units)\n","        return tf.transpose(outputs, [1, 0, 2])  # output with correct shape (batch_size, seq_len, units)"]},{"cell_type":"markdown","metadata":{"id":"vd4neYqVm_mB"},"source":["Finally, we take our `SimpleRNN` class and build it into a full model to predict the output. The class defining the AXBY model could look like this:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMJ4OFkXm-g7"},"outputs":[],"source":["class AXBY(tf.keras.Model):\n","\n","    def __init__(self, rnn_units):\n","        super().__init__()\n","        self.rnn = SimpleRNN(units=rnn_units)\n","        self.dense = tf.keras.layers.Dense(units=2, activation='sigmoid')\n","\n","    def call(self, inputs):\n","        x = self.rnn(inputs)\n","        return self.dense(x)\n","\n","model = AXBY(rnn_units=10)\n","\n","print(f\"{model(batch_x).shape=}\")\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"U8CWI5f83B74"},"source":["The new model is doing something very similar to the previous code in `easy_model`, but since we have defined the core classes ourselves, it is easier to see exactly what is going on in each layer, and potentially easier to modify or tailor the functionality of recurrent layer."]},{"cell_type":"markdown","metadata":{"id":"QZq3NdCMm_mC"},"source":["## Training the custom model"]},{"cell_type":"markdown","metadata":{"id":"pQ6AYPGJ3B75"},"source":["To better understand how a training loop might work, let's build it for the custom model using a lower-level API."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-fVNU55KKx9"},"outputs":[],"source":["# me"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t7R1oV4N3B76"},"outputs":[],"source":["loss_fn = tf.keras.losses.BinaryCrossentropy()\n","optimizer = tf.keras.optimizers.SGD(learning_rate=1)\n","\n","@tf.function  # Decorator to compile the function into a graph, and make it run faster\n","def train_step(x, y):  # x and y in this case are the inputs and targets of the model\n","    with tf.GradientTape() as tape:  # Record operations for automatic differentiation\n","        y_pred = model(x, training=True)  # Forward pass\n","        loss_value = loss_fn(y, y_pred)  # Compute the loss value\n","    grads = tape.gradient(loss_value, model.trainable_weights)  # get gradients: dL/dW in all trainable weights\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))  # Update all trainable weights and biases\n","    return loss_value  # Return the loss value so we can print it later"]},{"cell_type":"markdown","metadata":{"id":"I_yKZ_Uym-g8"},"source":["Advanced topic: Learn from Andrej Karpathy about how modern machine learning frameworks magically get the gradients (reverse-mode autodiff) [here](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1&t=492s)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuI13EsBm-g9"},"outputs":[],"source":["# model.fit() equivalent\n","# Train with the same data for 100 epochs\n","for epoch in range(epochs):\n","    loss_value = train_step(x_train, y_train)\n","    print(f\"Epoch {epoch + 1}: loss = {loss_value}\")"]},{"cell_type":"markdown","metadata":{"id":"hIF5XmAgm-g9"},"source":["We are using a loss function called _binary crossentropy_. Additionally we are using _stoachastic gradient descent_ as the optimizer, and printing out performance using our measure for loss. You can, of course, measure performance in any way you'd like, not just with the loss function. Different metrics have different quantitative structure and therefore offer different descriptive characteristics, so you can decide on one (or more) that work well for your purpose."]},{"cell_type":"markdown","metadata":{"id":"qWFpMfdHm-g9"},"source":["## Evaluate the results"]},{"cell_type":"markdown","metadata":{"id":"8E4w28v2m-g9"},"source":["There are a number of ways that you might evaluate the performance of the model. The method that you choose with a model that you develop will be unique to your particular application and your analytic goals.\n","\n","Here we provide a workflow that allows you to decode x and y so that you can _visually_ inspect the results. This is often useful when the output can be coerced into a human readable format of some kind."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbhlLnHzm_mC"},"outputs":[],"source":["import numpy as np\n","\n","def decode_y(y, is_prediction=False):\n","    \"\"\"Decode output from one-hot encoding to a list of number.\"\"\"\n","\n","    if isinstance(y, tf.Tensor):\n","        y = y.numpy()\n","\n","    if is_prediction:\n","        # y_pred is float, so we need to round it to 0 or 1\n","        if len(y) != 1:\n","            raise ValueError(f\"Prediction should be a single case, but got {len(y)} cases.\")\n","        y = y[0].round(0)\n","\n","    decoded = []\n","    for output in y:\n","        \n","        if all(output == [0, 0]):\n","            decoded.append(0)\n","        elif all(output == [1, 0]):\n","            decoded.append(1)\n","        elif all(output == [0, 1]):\n","            decoded.append(2)\n","        else:\n","            raise ValueError(f\"Invalid y: {output}\")\n","\n","    return decoded"]},{"cell_type":"markdown","metadata":{"id":"--u1fSWvm-g-"},"source":["Here we evaluate a random case:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"flYMSB56m-g-"},"outputs":[],"source":["def evaluate(model):\n","    \"\"\"Randomly generate a case and evaluate the expected outputs vs. model's prediction.\"\"\"\n","\n","    x, y = get_cases(n=1, verbose=True)\n","    pred_y = model(x)\n","    print(f\"Model prediction: {decode_y(pred_y, is_prediction=True)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stvx8_qKm-g_"},"outputs":[],"source":["evaluate(model)"]},{"cell_type":"markdown","metadata":{"id":"ZGzN0HtUm-g_"},"source":["The performance of the model is not perfect, but it does alright (especially for button 1). Notice that button 2 is way more difficult to learn due to fewer examples in the training set. You can also think through yourself what would allow this model to learn better. What aspects of the model structure and specification here could be augmented to enhance performance?\n","\n","Exercises:\n","\n","- Try to improve the accuracy by any means you can think of."]},{"cell_type":"markdown","metadata":{"id":"q1_PvfX5dI-S"},"source":["## Summary\n","So, in this module you've learned about simple recurrent neural networks, how to specify an RNN cell (single timestep) and fold more than one cell into an architecture that can handle multiple timesteps. Additionally, you have learned about how to apply the model in training and evaluation routines, along with some experience using the special jargon we use when talking about artificial neural networks.\n","\n","Next we will move on to a more complex model RNN architecture that uses _continuous_ computation, though we will build on many of the concepts we've covered here."]}],"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"tf-gpu","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"f55368bc7be0f2238dbef0a99c4d07d77945e5a459017dc2b141ca990cb81374"}}},"nbformat":4,"nbformat_minor":0}