{"cells":[{"cell_type":"markdown","metadata":{"id":"-QfW4Ocz3GwQ"},"source":["# Module 7: Network damage\n","\n","In the example, we will try different ways to damage PMSP model.\n","\n","Damage API (e.g., `model.cut_connection`) is only available in the models imported from `connectionist.models`.\n","\n","The implementation details in this module is out of the scope of this course. Advanced users can refer to the source code for details.\n","\n","\n","1. Remove connections between any two layers: `model.cut_connections()`\n","\n","2. Shrink the number of neurons in a layer: `model.shrink_layer()`\n","\n","3. Add noise to a layer: `model.add_noise()`\n","    \n","4. Apply pressure to keep weights small: `model.apply_l2()`\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tw5Rv13-3C2N"},"outputs":[],"source":["!pip install connectionist"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cfunC1dj3JEu"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","from connectionist.data import ToyOP\n","from connectionist.models import PMSP"]},{"cell_type":"markdown","metadata":{"id":"tEkPRfal3KpK"},"source":["## Data\n","\n","Identical to last module.\n","\n","- Input: word representations, fixed across time. \n","- Output: letter representations, changing across time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qcNCJ-_n3Lnn"},"outputs":[],"source":["data = ToyOP()\n","letters, words = data.letters, data.words\n","x_train, y_train = data.x_train, data.y_train"]},{"cell_type":"markdown","metadata":{"id":"JZQi03ut3MoT"},"source":["## Model creation and training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dvzzOOd43Nb7"},"outputs":[],"source":["model = PMSP(tau=0.1, h_units=10, p_units=9, c_units=5)\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.05), \n","    loss=tf.keras.losses.BinaryCrossentropy()\n",")\n","\n","model.fit(x_train, y_train, epochs=100, batch_size=20)"]},{"cell_type":"markdown","metadata":{"id":"fpxpCY8Q3TZ1"},"source":["## Model predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SKsCpaD-3PYk"},"outputs":[],"source":["def decode_prediction(y, idx=None):\n","    \"\"\"Decode the prediction.\"\"\"\n","\n","    decoded = ''.join([letters[np.argmax(v)] for v in y])\n","    return decoded\n","\n","y_pred = model(x_train)\n","\n","for i, word in enumerate(words):\n","    print(f\"word: {word}; pred: {decode_prediction(y_pred['phonology'][i])}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m9f4UYfPygWO"},"outputs":[],"source":["y_pred.keys()"]},{"cell_type":"markdown","metadata":{"id":"GuIrbwji3YcF"},"source":["## Damage I: Remove a connection (weights) between two layers"]},{"cell_type":"markdown","metadata":{"id":"WeOf9Kuv_s5G"},"source":["Implementation summary:\n","\n","1. Remove the specified cut `connections` from the original `model.connections` -> `new_connections`\n","2. Rebuild empty new model with new `new_connections`\n","3. Transplant all weights and biases"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CYVj2Y2q3ZR4"},"outputs":[],"source":["# API interface draft\n","# Assignment to a new variable is required (due to scoping issues)\n","\n","# Design choice: for simplicity I use singular weight here . User can use a simple for loop to cut multiple connections.\n","new_model = model.cut_connections(connections=['cp'])"]},{"cell_type":"markdown","metadata":{"id":"N4yxx14x7wEk"},"source":["You can see the trainable weights in the model includes `cp`, but the new one does not."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUyhXL-X7wEk"},"outputs":[],"source":["[w.name for w in model.trainable_weights]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7e8Gje07wEk"},"outputs":[],"source":["[w.name for w in new_model.trainable_weights]"]},{"cell_type":"markdown","metadata":{"id":"ffEt9B04G8Gq"},"source":["## Damage II: Zero out some units in a weight matrix\n","\n","Steps:\n","1. Locate weight matrix\n","2. Create elementwise un-trainable mask according to the rate: \n","\n","    $p(v = 0) = rate$, $p(v = 1) = 1 - rate$, $rate \\in [0, 1)$ where \n","    $v$ is the value of the mask, and $rate$ is the rate of zeroing out. \n","\n","3. Zero out the actual weights by the mask (to get-rid of initialized random values):\n","\n","    $w = w \\times mask$\n","\n","4. Mask the original weights in forward pass, i.e.: \n","\n","    $w_{masked} = w \\times mask$\n","\n","\n","With this implementation, backward-pass will return zero gradient in the masked unit (due to step 4). More specifically, during back propagation, the gradient is calculated as:\n","\n","$d_w = mask \\times d_{w_{masked}}$\n","\n","$mask = 0 \\rightarrow d_w = 0$\n","\n","No training signal can back-propagate through the mask."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHg0L3vO7wEl"},"outputs":[],"source":["new_model = model.zero_out(rates={'pp': 0.2})"]},{"cell_type":"markdown","metadata":{"id":"LZhJRqKi7wEl"},"source":["We can see all the masks by calling:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IGUq4jkn7wEm"},"outputs":[],"source":["[w.name for w in new_model.weights if 'zero_out_mask' in w.name]"]},{"cell_type":"markdown","metadata":{"id":"F1q1ZCiu7wEm"},"source":["Most of them are inclusive mask, with only values of 1.0, for example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUQsgSsT7wEn"},"outputs":[],"source":["new_model.pmsp.cell.oh.zero_out_mask"]},{"cell_type":"markdown","metadata":{"id":"aPIz8OVK7wEn"},"source":["More importantly, `pp` mask is the one we had set to zero out 20% of the weights:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMXuj_Xf7wEn"},"outputs":[],"source":["new_model.pmsp.cell.pp.zero_out_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxe7ajAh9HKo"},"outputs":[],"source":["print(f\"Actual zeroed-out rate = {1-tf.reduce_mean(new_model.pmsp.cell.pp.zero_out_mask).numpy():.03f}\")"]},{"cell_type":"markdown","metadata":{"id":"XRVjrQgd7wEn"},"source":["Also, the weights are zeroed out at where the mask is zero:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U4p0r0wP7wEn"},"outputs":[],"source":["new_model.pmsp.cell.pp.kernel  # kernel means weights in KERAS terminology"]},{"cell_type":"markdown","metadata":{"id":"ARV2h9Uv7wEn"},"source":["To show the zeroed out weights are behaving like un-trainable weights, we can try to train the new model further:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wCKjfBVM7wEo"},"outputs":[],"source":["new_model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.05), \n","    loss=tf.keras.losses.BinaryCrossentropy()\n",")\n","new_model.fit(x_train, y_train, epochs=100, batch_size=20, verbose=0)"]},{"cell_type":"markdown","metadata":{"id":"dGwdWQ4v7wEo"},"source":["They are indeed un-trainable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6b97zdE7wEo"},"outputs":[],"source":["new_model.pmsp.cell.pp.kernel"]},{"cell_type":"markdown","metadata":{"id":"pM5RCYR-3aF6"},"source":["## Damage III: Remove a portion of neurons in a layer"]},{"cell_type":"markdown","metadata":{"id":"35hqwUolAZeG"},"source":["Implementation summary:\n","\n","1. Calculate the new number of units: `n = int((1-rate) * original units)`\n","1. Random sample `n` from range(n) to get a list of index `i`\n","1. Depending on the shrink_layer's `layer` argument, look for all incoming and outgoing connections (weights and biases), subset the axis that link to the target layer by `i`\n","1. Create new model and transplant "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mC5W4tlT3bZH"},"outputs":[],"source":["new_model = model.shrink_layer(layer='hidden', rate=0.3)  # shrinking hidden layer by 30%"]},{"cell_type":"markdown","metadata":{"id":"DP9UXvs-3cHa"},"source":["## Damage IV: Add noise to a layer"]},{"cell_type":"markdown","metadata":{"id":"7RU3DsgfIhu7"},"source":["Notes:\n","- Noise apply during training and inference\n","- Noise is **added** instead of replacing to old noise. e.g., orignal p_noise = 0.1, running add_noise to phonology with 0.1 stddev will cause the new model to have p_noise = 0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r7DlZ68d3c7U"},"outputs":[],"source":["new_model = model.add_noise(layer='phonology', stddev=0.1)"]},{"cell_type":"markdown","metadata":{"id":"wkF3wXYA7wEq"},"source":["Manually locating phonology noise layer and test with some ones:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2R-zhxeJ7wEq"},"outputs":[],"source":["new_model.pmsp.cell.noise['phonology'](tf.ones((3,3)), training=True)"]},{"cell_type":"markdown","metadata":{"id":"amSBgZ1E7wEq"},"source":["Note that inside `PMSPCell.call()`, the noise layer is locked to `training=True`, where noise is added in both training and inference."]},{"cell_type":"markdown","metadata":{"id":"P5M0g_4Y3d1P"},"source":["## Damage V: Apply pressure to keep the weights small\n","\n","Weight decay and L2 regularization. \n","\n","Useful details: [blog post about weight decay vs. L2 regularization](https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd)\n","\n","Worth mentioning: Why lesioning sounds bad but regularization sounds good? Perhaps, things don't kill you makes you stronger (more fault tolerant).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y4honQ23_cBn"},"outputs":[],"source":["new_model = model.apply_l2(l2=0.3)"]},{"cell_type":"markdown","metadata":{"id":"gKwRUglO_foq"},"source":["Train the new model to see the impact of adding L2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1P9KFOWszXp5"},"outputs":[],"source":["new_model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.05), \n","    loss=tf.keras.losses.BinaryCrossentropy()\n",")\n","new_model.fit(x_train, y_train, epochs=20, batch_size=20)"]},{"cell_type":"markdown","metadata":{"id":"lG1p8z6m7wEs"},"source":["Let's check if the weights' magnitude are decreased"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXxtEcU37wEs"},"outputs":[],"source":["[f\"{w.name}: {tf.reduce_sum(w)}\" for w in model.trainable_weights]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xAiVGE9j7wEs"},"outputs":[],"source":["[f\"{w.name}: {tf.reduce_sum(w)}\" for w in new_model.trainable_weights]"]},{"cell_type":"markdown","metadata":{"id":"2Wx_52u17wEs"},"source":["Explaining the details using a custom training loop:"]},{"cell_type":"markdown","metadata":{"id":"1fJ5Cy4zzdeR"},"source":["\n","```python\n","# Define train step with l2\n","def train_step(self, x, y, l2=0.0):\n","    with tf.GradientTape() as tape:\n","        y_pred = self(x, training=True)\n","        loss = self.loss(y, y_pred)\n","        loss += l2 * tf.reduce_sum(tf.square(self.trainable_weights))  # Add L2 regularization manually\n","    grads = tape.gradient(loss, self.trainable_weights)\n","    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n","    return loss\n","\n","# Train\n","for _ in range(epochs):\n","    train_step(x_train, y_train)\n","\n","```"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"tf-gpu","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"f55368bc7be0f2238dbef0a99c4d07d77945e5a459017dc2b141ca990cb81374"}}},"nbformat":4,"nbformat_minor":0}